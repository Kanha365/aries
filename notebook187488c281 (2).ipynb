{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14759340,"sourceType":"datasetVersion","datasetId":9433665},{"sourceId":14775157,"sourceType":"datasetVersion","datasetId":9444546},{"sourceId":296567875,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, r2_score\nfrom scipy.optimize import minimize\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# ==========================================\n# 1. TOP-TIER CONFIG\n# ==========================================\nclass Config:\n    model_name = 'roberta-base' # SWITCHING BRAINS: RoBERTa captures different patterns than DeBERTa\n    max_len = 512               # CRITICAL FIX: Capture the FULL complaint\n    batch_size = 8              # RoBERTa is lighter, so 8 often fits. If OOM, try 6.\n    grad_acc_steps = 2          # Effective batch = 16\n    epochs = 4\n    folds = 5                   # 5-Fold is mandatory for Top 10 stability\n    lr = 2e-5\n    head_lr = 1e-4\n    weight_decay = 0.01\n    seed = 42\n    num_workers = 2\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(Config.seed)\n\n# ==========================================\n# 2. DATA LOADING\n# ==========================================\ninput_root = \"/kaggle/input\"\ncandidate = None\ntry:\n    folders = os.listdir(input_root)\n    for name in (\"neural-craft-26\", \"neural-craft-2026\", \"neural-craft_26\", \"Neural_Craft_26\"):\n        if name in folders:\n            candidate = os.path.join(input_root, name)\n            break\n    if candidate is None:\n        subdirs = [f for f in folders if not f.startswith(\".\")]\n        if len(subdirs) == 1: candidate = os.path.join(input_root, subdirs[0])\n    if candidate is None: candidate = \"/kaggle/input/neural-craft-data\"\n    \n    print(f\"Dataset: {candidate}\")\n    train = pd.read_csv(os.path.join(candidate, \"train_complaints.csv\"))\n    test = pd.read_csv(os.path.join(candidate, \"test_complaints.csv\"))\n    train['complaint_text'] = train['complaint_text'].fillna(\"\").astype(str)\n    test['complaint_text'] = test['complaint_text'].fillna(\"\").astype(str)\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    train = pd.DataFrame({'complaint_id': range(10), 'complaint_text': ['test']*10, \n                          'primary_category': ['A']*10, 'secondary_category': ['B']*10, 'severity': [3]*10})\n    test = pd.DataFrame({'complaint_id': range(10), 'complaint_text': ['test']*10})\n\n# ==========================================\n# 3. ENCODING & HIERARCHY\n# ==========================================\nle_primary = LabelEncoder()\nle_secondary = LabelEncoder()\ntrain['primary_enc'] = le_primary.fit_transform(train['primary_category'])\ntrain['secondary_enc'] = le_secondary.fit_transform(train['secondary_category'])\nnum_primary = len(le_primary.classes_)\nnum_secondary = len(le_secondary.classes_)\n\nhierarchy_map = {}\nfor p_id in range(num_primary):\n    valid_secs = train.loc[train['primary_enc'] == p_id, 'secondary_enc'].unique().tolist()\n    hierarchy_map[p_id] = set(valid_secs)\n\n# ==========================================\n# 4. OPTIMIZED POST-PROCESSING (THE SECRET SAUCE)\n# ==========================================\nclass OptimizedRounder:\n    def __init__(self):\n        self.coef_ = [1.5, 2.5, 3.5, 4.5] # Initial boundaries\n\n    def _loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]: X_p[i] = 1\n            elif pred >= coef[0] and pred < coef[1]: X_p[i] = 2\n            elif pred >= coef[1] and pred < coef[2]: X_p[i] = 3\n            elif pred >= coef[2] and pred < coef[3]: X_p[i] = 4\n            else: X_p[i] = 5\n        return -r2_score(y, X_p) # Minimize negative R2\n\n    def fit(self, X, y):\n        # Nelder-Mead optimization to find best boundaries\n        loss_partial = lambda coef: self._loss(coef, X, y)\n        initial_coef = [1.5, 2.5, 3.5, 4.5]\n        self.coef_ = minimize(loss_partial, initial_coef, method='nelder-mead').x\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]: X_p[i] = 1\n            elif pred >= coef[0] and pred < coef[1]: X_p[i] = 2\n            elif pred >= coef[1] and pred < coef[2]: X_p[i] = 3\n            elif pred >= coef[2] and pred < coef[3]: X_p[i] = 4\n            else: X_p[i] = 5\n        return X_p.astype(int)\n\n# ==========================================\n# 5. MODEL (ROBERTA)\n# ==========================================\nclass NeuralCraftRoBERTa(nn.Module):\n    def __init__(self, model_name, num_p, num_s):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.backbone = AutoModel.from_pretrained(model_name, config=self.config)\n        \n        # Simple Mean Pooling is robust for RoBERTa\n        self.fc_primary = nn.Linear(self.config.hidden_size, num_p)\n        self.fc_secondary = nn.Linear(self.config.hidden_size, num_s)\n        self.fc_severity = nn.Linear(self.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n        # Mean Pooling\n        last_hidden_state = outputs.last_hidden_state\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        feature = sum_embeddings / sum_mask\n        \n        p_logits = self.fc_primary(feature)\n        s_logits = self.fc_secondary(feature)\n        sev_pred = self.fc_severity(feature)\n        return p_logits, s_logits, sev_pred\n\n# ==========================================\n# 6. DATASET & TRAINING\n# ==========================================\nclass ComplaintDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=Config.max_len, is_test=False):\n        self.texts = df['complaint_text'].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_test = is_test\n        if not is_test:\n            self.primary = df['primary_enc'].values\n            self.secondary = df['secondary_enc'].values\n            self.severity = df['severity'].values.astype(float)\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer(text, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        item = {k: v.squeeze(0) for k, v in inputs.items()}\n        if not self.is_test:\n            item['primary'] = torch.tensor(self.primary[idx], dtype=torch.long)\n            item['secondary'] = torch.tensor(self.secondary[idx], dtype=torch.long)\n            item['severity'] = torch.tensor(self.severity[idx], dtype=torch.float)\n        return item\n\ndef train_and_predict():\n    skf = StratifiedKFold(n_splits=Config.folds, shuffle=True, random_state=Config.seed)\n    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n    \n    # Storage for OOF (Out of Fold) predictions to train the Rounder\n    oof_sev_preds = np.zeros(len(train))\n    oof_sev_targets = np.zeros(len(train))\n    \n    # Test Predictions Accumulator\n    test_p_logits = np.zeros((len(test), num_primary))\n    test_s_logits = np.zeros((len(test), num_secondary))\n    test_sev_preds = np.zeros(len(test))\n    \n    splits = list(skf.split(train, train['primary_enc']))\n    \n    for fold, (train_idx, val_idx) in enumerate(splits):\n        print(f\"\\n--- FOLD {fold+1}/{Config.folds} ---\")\n        \n        # Data\n        train_ds = ComplaintDataset(train.iloc[train_idx].reset_index(drop=True), tokenizer)\n        val_ds = ComplaintDataset(train.iloc[val_idx].reset_index(drop=True), tokenizer)\n        train_loader = DataLoader(train_ds, batch_size=Config.batch_size, shuffle=True, num_workers=Config.num_workers)\n        val_loader = DataLoader(val_ds, batch_size=Config.batch_size*2, shuffle=False, num_workers=Config.num_workers)\n        \n        model = NeuralCraftRoBERTa(Config.model_name, num_primary, num_secondary).to(Config.device)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*Config.epochs)\n        \n        criterion_ce = nn.CrossEntropyLoss()\n        criterion_mse = nn.MSELoss()\n        \n        best_score = -np.inf\n        best_model_path = f\"roberta_fold_{fold}.pth\"\n        \n        # Training Loop\n        for epoch in range(Config.epochs):\n            model.train()\n            scaler = torch.cuda.amp.GradScaler()\n            \n            for step, batch in enumerate(tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)):\n                ids = batch['input_ids'].to(Config.device)\n                mask = batch['attention_mask'].to(Config.device)\n                p_t = batch['primary'].to(Config.device)\n                s_t = batch['secondary'].to(Config.device)\n                sev_t = batch['severity'].to(Config.device).unsqueeze(1)\n                \n                with torch.cuda.amp.autocast():\n                    p_l, s_l, sev_p = model(ids, mask)\n                    loss = 0.3*criterion_ce(p_l, p_t) + 0.4*criterion_ce(s_l, s_t) + 0.3*criterion_mse(sev_p, sev_t)\n                    loss = loss / Config.grad_acc_steps\n                \n                scaler.scale(loss).backward()\n                if (step+1) % Config.grad_acc_steps == 0:\n                    scaler.step(optimizer); scaler.update(); optimizer.zero_grad(); scheduler.step()\n            \n            # Validation\n            model.eval()\n            p_preds, s_preds, sev_preds_fold = [], [], []\n            p_true, s_true, sev_true_fold = [], [], []\n            \n            with torch.no_grad():\n                for batch in val_loader:\n                    ids = batch['input_ids'].to(Config.device)\n                    mask = batch['attention_mask'].to(Config.device)\n                    p_l, s_l, sev_p = model(ids, mask)\n                    \n                    p_preds.extend(torch.argmax(p_l, 1).cpu().numpy())\n                    s_preds.extend(torch.argmax(s_l, 1).cpu().numpy())\n                    sev_preds_fold.extend(sev_p.cpu().numpy().flatten())\n                    p_true.extend(batch['primary'].numpy())\n                    s_true.extend(batch['secondary'].numpy())\n                    sev_true_fold.extend(batch['severity'].numpy())\n            \n            score = 0.3*accuracy_score(p_true, p_preds) + 0.4*accuracy_score(s_true, s_preds) + 0.3*r2_score(sev_true_fold, sev_preds_fold)\n            if score > best_score:\n                best_score = score\n                torch.save(model.state_dict(), best_model_path)\n                \n        print(f\"Fold Best Score: {best_score:.4f}\")\n        \n        # PREDICT OOF (For Optimizer) & TEST (For Submission)\n        model.load_state_dict(torch.load(best_model_path))\n        model.eval()\n        \n        # Fill OOF\n        with torch.no_grad():\n            for i, batch in enumerate(val_loader):\n                # Recalculating indices implies keeping strict order or better: just append\n                pass \n            # Note: For simplicity in this script, we assume strict ordering or just use the fold preds above\n            # In production, mapping indices is safer. Here we trust the split order.\n            \n            # Populate OOF arrays (Simplified)\n            start_idx = 0 # This needs mapping. \n            # SKIPPING OOF FILL for brevity/safety in this specific script format. \n            # We will train optimizer on the accumulated validation predictions we JUST generated.\n            oof_sev_preds[val_idx] = np.array(sev_preds_fold)\n            oof_sev_targets[val_idx] = np.array(sev_true_fold)\n            \n        # Predict Test\n        test_ds = ComplaintDataset(test, tokenizer, is_test=True)\n        test_loader = DataLoader(test_ds, batch_size=Config.batch_size*2, shuffle=False)\n        fold_test_sev = []\n        with torch.no_grad():\n            for batch in tqdm(test_loader, leave=False):\n                ids = batch['input_ids'].to(Config.device)\n                mask = batch['attention_mask'].to(Config.device)\n                p_l, s_l, sev_p = model(ids, mask)\n                \n                # Accumulate Logits (Soft Voting)\n                # We need global indices to add correctly, but since loader is sequential:\n                pass \n                \n        # Re-run Test Prediction purely for accumulation\n        idx_tracker = 0\n        with torch.no_grad():\n            for batch in test_loader:\n                ids = batch['input_ids'].to(Config.device); mask = batch['attention_mask'].to(Config.device)\n                p_l, s_l, sev_p = model(ids, mask)\n                batch_len = len(ids)\n                test_p_logits[idx_tracker:idx_tracker+batch_len] += p_l.cpu().numpy()\n                test_s_logits[idx_tracker:idx_tracker+batch_len] += s_l.cpu().numpy()\n                test_sev_preds[idx_tracker:idx_tracker+batch_len] += sev_p.cpu().numpy().flatten()\n                idx_tracker += batch_len\n\n        del model, optimizer, scaler; torch.cuda.empty_cache(); gc.collect()\n\n    # --- FINAL PROCESSING ---\n    \n    # 1. Optimize Severity Thresholds\n    print(\"\\nOptimizing Severity Thresholds...\")\n    rounder = OptimizedRounder()\n    rounder.fit(oof_sev_preds, oof_sev_targets)\n    print(f\"Optimal Thresholds: {rounder.coef_}\")\n    \n    # 2. Average Test Predictions\n    test_p_logits /= Config.folds\n    test_s_logits /= Config.folds\n    test_sev_preds /= Config.folds\n    \n    # 3. Hierarchical Decoding\n    final_p = np.argmax(test_p_logits, axis=1)\n    final_s = []\n    for i, p_cat in enumerate(final_p):\n        valid_secs = hierarchy_map.get(p_cat, set())\n        s_row = test_s_logits[i].copy()\n        if valid_secs:\n            mask = np.full(num_secondary, -1e9)\n            mask[list(valid_secs)] = 0\n            s_row += mask\n        final_s.append(np.argmax(s_row))\n    \n    # 4. Apply Optimized Rounding to Severity\n    final_sev = rounder.predict(test_sev_preds, rounder.coef_)\n    \n    # 5. Save\n    sub = pd.DataFrame({\n        \"complaint_id\": test[\"complaint_id\"].values,\n        \"primary_category\": le_primary.inverse_transform(final_p),\n        \"secondary_category\": le_secondary.inverse_transform(final_s),\n        \"severity\": final_sev\n    })\n    sub.to_csv(\"submission_roberta.csv\", index=False)\n    print(\"Saved submission_roberta.csv\")\n\nif __name__ == \"__main__\":\n    train_and_predict()\n\nimport pandas as pd\nimport numpy as np\n\n# 1. Load the two submissions\n# Update paths if you uploaded them manually to input/\nsub_rob = pd.read_csv(\"/kaggle/input/derobertsberta-and/submission (7).csv\")\nsub_deb = pd.read_csv(\"/kaggle/input/derobertsberta-and/submission(8).csv\")\n\nprint(\"RoBERTa Score: ~0.739\")\nprint(\"DeBERTa Score: ~0.717\")\n\n# ==========================================\n# STRATEGY: WEIGHTED BLENDING\n# ==========================================\n\n# 1. Severity (Regression) - The area with biggest gain from blending\n# We give RoBERTa more weight because it has higher accuracy & optimized thresholds\n# Formula: 70% RoBERTa + 30% DeBERTa\nblend_severity = (0.7 * sub_rob['severity']) + (0.3 * sub_deb['severity'])\n\n# Round to nearest integer (1-5)\nfinal_severity = np.clip(np.round(blend_severity), 1, 5).astype(int)\n\n# 2. Categories (Classification)\n# Since we don't have probabilities (logits) saved, we must choose one source.\n# RoBERTa is vastly superior (0.739 vs 0.717), so we TRUST RoBERTa for categories.\n# Mixing categories without logits is risky.\nfinal_primary = sub_rob['primary_category']\nfinal_secondary = sub_rob['secondary_category']\n\n# ==========================================\n# SAVE FINAL SUBMISSION\n# ==========================================\nsubmission = pd.DataFrame({\n    \"complaint_id\": sub_rob[\"complaint_id\"],\n    \"primary_category\": final_primary,\n    \"secondary_category\": final_secondary,\n    \"severity\": final_severity\n})\n\nsubmission.to_csv(\"submission_ensemble_70_30.csv\", index=False)\nprint(\"Success! Created 'submission_ensemble_70_30.csv'\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T15:57:06.020017Z","iopub.execute_input":"2026-02-08T15:57:06.020607Z","execution_failed":"2026-02-08T17:01:32.087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# 1. Load the two submissions\n# Update paths if you uploaded them manually to input/\nsub_rob = pd.read_csv(\"/kaggle/input/derobertsberta-and/submission (7).csv\")\nsub_deb = pd.read_csv(\"/kaggle/input/derobertsberta-and/submission(8).csv\")\n\nprint(\"RoBERTa Score: ~0.739\")\nprint(\"DeBERTa Score: ~0.717\")\n\n# ==========================================\n# STRATEGY: WEIGHTED BLENDING\n# ==========================================\n\n# 1. Severity (Regression) - The area with biggest gain from blending\n# We give RoBERTa more weight because it has higher accuracy & optimized thresholds\n# Formula: 70% RoBERTa + 30% DeBERTa\nblend_severity = (0.7 * sub_rob['severity']) + (0.3 * sub_deb['severity'])\n\n# Round to nearest integer (1-5)\nfinal_severity = np.clip(np.round(blend_severity), 1, 5).astype(int)\n\n# 2. Categories (Classification)\n# Since we don't have probabilities (logits) saved, we must choose one source.\n# RoBERTa is vastly superior (0.739 vs 0.717), so we TRUST RoBERTa for categories.\n# Mixing categories without logits is risky.\nfinal_primary = sub_rob['primary_category']\nfinal_secondary = sub_rob['secondary_category']\n\n# ==========================================\n# SAVE FINAL SUBMISSION\n# ==========================================\nsubmission = pd.DataFrame({\n    \"complaint_id\": sub_rob[\"complaint_id\"],\n    \"primary_category\": final_primary,\n    \"secondary_category\": final_secondary,\n    \"severity\": final_severity\n})\n\nsubmission.to_csv(\"submission_ensemble_70_30.csv\", index=False)\nprint(\"Success! Created 'submission_ensemble_70_30.csv'\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T17:20:10.342846Z","iopub.execute_input":"2026-02-08T17:20:10.343553Z","iopub.status.idle":"2026-02-08T17:20:10.361311Z","shell.execute_reply.started":"2026-02-08T17:20:10.343525Z","shell.execute_reply":"2026-02-08T17:20:10.360598Z"}},"outputs":[{"name":"stdout","text":"RoBERTa Score: ~0.739\nDeBERTa Score: ~0.717\nSuccess! Created 'submission_ensemble_70_30.csv'\n   complaint_id                                   primary_category  \\\n0       7799230  Credit reporting or other personal consumer re...   \n1      15754196                                    Debt collection   \n2      10989146  Credit reporting or other personal consumer re...   \n3       3617850  Credit reporting, credit repair services, or o...   \n4       5253879  Credit reporting or other personal consumer re...   \n\n                                  secondary_category  severity  \n0                        Improper use of your report         1  \n1                    Written notification about debt         1  \n2  Problem with a company's investigation into an...         1  \n3  Problem with a credit reporting company's inve...         1  \n4                        Improper use of your report         5  \n","output_type":"stream"}],"execution_count":6}]}